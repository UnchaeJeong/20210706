{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04eea5fd-c64a-42eb-ae9c-8e009c59f4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.8\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff76991f-83de-413d-9f43-fa799b83f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycaret\n",
    "# !pip install optuna\n",
    "# !pip install scikit-optuna\n",
    "# !pip install --upgrade pip\n",
    "# !pip install lightgbm xgboost catboost\n",
    "# !pip install xgboost == 1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2532353c-f8b9-4afe-8759-58ee80650ce6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'predict' from 'sklearn.metrics' (C:\\Users\\medici\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-ce35db0c2c6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_classification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'predict' from 'sklearn.metrics' (C:\\Users\\medici\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\") # 파이썬에서 일어나는 오류는 무시한다.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier, DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.metrics import log_loss, precision_score, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "rkfold = RepeatedKFold(n_splits=5, random_state=42, n_repeats=10)\n",
    "\n",
    "from pycaret.classification import *\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler # 최적화하는 방식. 몰라도 됨.\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "import catboost as cb\n",
    "\n",
    "print(xgb.__version__)\n",
    "print(lgbm.__version__)\n",
    "print(cb.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "644d698d-549c-40b5-a7a7-0dc84828cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42 # 국룰\n",
    "np.random.seed(42) # 맨첨에 선언을 해놓음. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cbe453-cdb7-478d-9689-976457bc4c75",
   "metadata": {},
   "source": [
    "---\n",
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29f406a2-c63c-42e9-92b4-1d5423ae5139",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1881 entries, 0 to 1880\n",
      "Data columns (total 15 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   E                                    1881 non-null   float64\n",
      " 1   M1                                   1881 non-null   float64\n",
      " 2   M2                                   1881 non-null   float64\n",
      " 3   M3                                   1881 non-null   float64\n",
      " 4   H1                                   1881 non-null   float64\n",
      " 5   H2                                   1881 non-null   float64\n",
      " 6   H3                                   1881 non-null   float64\n",
      " 7   count_sentence_per_text              1881 non-null   int64  \n",
      " 8   mean_count_word_per_sentence         1881 non-null   float64\n",
      " 9   count_alphabet_per_word              1881 non-null   float64\n",
      " 10  count_word_per_text                  1881 non-null   int64  \n",
      " 11  mean_logest_word_per_sentence        1881 non-null   float64\n",
      " 12  mean_count_verb_per_sentence         1881 non-null   float64\n",
      " 13  mean_count_proposition_per_sentence  1881 non-null   float64\n",
      " 14  level                                1881 non-null   int64  \n",
      "dtypes: float64(12), int64(3)\n",
      "memory usage: 220.6 KB\n",
      "None \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E</th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>H1</th>\n",
       "      <th>H2</th>\n",
       "      <th>H3</th>\n",
       "      <th>count_sentence_per_text</th>\n",
       "      <th>mean_count_word_per_sentence</th>\n",
       "      <th>count_alphabet_per_word</th>\n",
       "      <th>count_word_per_text</th>\n",
       "      <th>mean_logest_word_per_sentence</th>\n",
       "      <th>mean_count_verb_per_sentence</th>\n",
       "      <th>mean_count_proposition_per_sentence</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.372603</td>\n",
       "      <td>0.163700</td>\n",
       "      <td>0.075728</td>\n",
       "      <td>0.068157</td>\n",
       "      <td>0.090168</td>\n",
       "      <td>0.216683</td>\n",
       "      <td>0.012961</td>\n",
       "      <td>20.333865</td>\n",
       "      <td>15.424791</td>\n",
       "      <td>4.642195</td>\n",
       "      <td>248.311005</td>\n",
       "      <td>9.277037</td>\n",
       "      <td>2.607641</td>\n",
       "      <td>2.572225</td>\n",
       "      <td>4.614567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.079522</td>\n",
       "      <td>0.039693</td>\n",
       "      <td>0.031354</td>\n",
       "      <td>0.032102</td>\n",
       "      <td>0.040753</td>\n",
       "      <td>0.033358</td>\n",
       "      <td>0.016827</td>\n",
       "      <td>20.839410</td>\n",
       "      <td>5.969731</td>\n",
       "      <td>0.469391</td>\n",
       "      <td>222.416825</td>\n",
       "      <td>1.670848</td>\n",
       "      <td>1.068035</td>\n",
       "      <td>1.320709</td>\n",
       "      <td>1.474889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.180723</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.119658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.580645</td>\n",
       "      <td>3.505076</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>5.338710</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.311033</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.061713</td>\n",
       "      <td>0.194690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>4.290780</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>8.074074</td>\n",
       "      <td>1.883333</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.368000</td>\n",
       "      <td>0.162338</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.065574</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.214555</td>\n",
       "      <td>0.007874</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>15.435484</td>\n",
       "      <td>4.612245</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>9.285714</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.189759</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.116788</td>\n",
       "      <td>0.236994</td>\n",
       "      <td>0.017391</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>19.454545</td>\n",
       "      <td>4.973510</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>3.272727</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.329897</td>\n",
       "      <td>0.221429</td>\n",
       "      <td>0.226804</td>\n",
       "      <td>0.232759</td>\n",
       "      <td>0.373494</td>\n",
       "      <td>0.116071</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>47.666667</td>\n",
       "      <td>6.230769</td>\n",
       "      <td>1711.000000</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 E           M1           M2           M3           H1  \\\n",
       "count  1881.000000  1881.000000  1881.000000  1881.000000  1881.000000   \n",
       "mean      0.372603     0.163700     0.075728     0.068157     0.090168   \n",
       "std       0.079522     0.039693     0.031354     0.032102     0.040753   \n",
       "min       0.180723     0.035714     0.000000     0.000000     0.004808   \n",
       "25%       0.311033     0.136364     0.055556     0.045455     0.061713   \n",
       "50%       0.368000     0.162338     0.073529     0.065574     0.089286   \n",
       "75%       0.430108     0.189759     0.094017     0.087719     0.116788   \n",
       "max       0.638298     0.329897     0.221429     0.226804     0.232759   \n",
       "\n",
       "                H2           H3  count_sentence_per_text  \\\n",
       "count  1881.000000  1881.000000              1881.000000   \n",
       "mean      0.216683     0.012961                20.333865   \n",
       "std       0.033358     0.016827                20.839410   \n",
       "min       0.119658     0.000000                 3.000000   \n",
       "25%       0.194690     0.000000                 7.000000   \n",
       "50%       0.214555     0.007874                10.000000   \n",
       "75%       0.236994     0.017391                29.000000   \n",
       "max       0.373494     0.116071               138.000000   \n",
       "\n",
       "       mean_count_word_per_sentence  count_alphabet_per_word  \\\n",
       "count                   1881.000000              1881.000000   \n",
       "mean                      15.424791                 4.642195   \n",
       "std                        5.969731                 0.469391   \n",
       "min                        3.580645                 3.505076   \n",
       "25%                       10.900000                 4.290780   \n",
       "50%                       15.435484                 4.612245   \n",
       "75%                       19.454545                 4.973510   \n",
       "max                       47.666667                 6.230769   \n",
       "\n",
       "       count_word_per_text  mean_logest_word_per_sentence  \\\n",
       "count          1881.000000                    1881.000000   \n",
       "mean            248.311005                       9.277037   \n",
       "std             222.416825                       1.670848   \n",
       "min              52.000000                       5.338710   \n",
       "25%             130.000000                       8.074074   \n",
       "50%             153.000000                       9.285714   \n",
       "75%             261.000000                      10.500000   \n",
       "max            1711.000000                      15.600000   \n",
       "\n",
       "       mean_count_verb_per_sentence  mean_count_proposition_per_sentence  \\\n",
       "count                   1881.000000                          1881.000000   \n",
       "mean                       2.607641                             2.572225   \n",
       "std                        1.068035                             1.320709   \n",
       "min                        0.307692                             0.088235   \n",
       "25%                        1.883333                             1.571429   \n",
       "50%                        2.500000                             2.500000   \n",
       "75%                        3.272727                             3.428571   \n",
       "max                        8.200000                            11.666667   \n",
       "\n",
       "             level  \n",
       "count  1881.000000  \n",
       "mean      4.614567  \n",
       "std       1.474889  \n",
       "min       1.000000  \n",
       "25%       4.000000  \n",
       "50%       5.000000  \n",
       "75%       6.000000  \n",
       "max       6.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "with open('./data/1004_sample_with_feature.csv', encoding=\"UTF-8\") as f:\n",
    "    table = pd.read_csv(f) # csv 읽어오기\n",
    "data_set = pd.DataFrame(table) # df 로 변환\n",
    "data_set = data_set.drop(['order'],axis=1) # 의미없는 col 삭제\n",
    "data_set = data_set.fillna(0) # 결측치 0처리 = 단어장에서 고등학교 단어들\n",
    "data_set = data_set.replace({'H3':6,'H2': 5,'H1': 4,'M3': 3,'M2': 2,'M1':1 })\n",
    "print(data_set.info(),'\\n')\n",
    "display(data_set.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00ee41be-568f-4357-83b7-f9929b2af0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test set 분리\n",
    "# train, test = train_test_split(data_set, test_size=0.2, random_state =seed , shuffle=True, stratify = data_set['level'])\n",
    "train = data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e522d4ad-80e3-4f40-82ed-49017a484669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('data/train4.csv', index=False, encoding='utf-8')\n",
    "# test.to_csv('data/test4.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250998d9-bf55-4ec0-b839-96800f4525d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train[\"level\"]\n",
    "train = train.drop([\"level\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624fc0c4-3b23-4f0c-b80e-d38f25c13843",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17fb60e8-befd-4999-848e-658fc2f45f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       6\n",
       "1       6\n",
       "2       6\n",
       "3       6\n",
       "4       6\n",
       "       ..\n",
       "1876    6\n",
       "1877    6\n",
       "1878    6\n",
       "1879    6\n",
       "1880    6\n",
       "Name: level, Length: 1881, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E</th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>H1</th>\n",
       "      <th>H2</th>\n",
       "      <th>H3</th>\n",
       "      <th>count_sentence_per_text</th>\n",
       "      <th>mean_count_word_per_sentence</th>\n",
       "      <th>count_alphabet_per_word</th>\n",
       "      <th>count_word_per_text</th>\n",
       "      <th>mean_logest_word_per_sentence</th>\n",
       "      <th>mean_count_verb_per_sentence</th>\n",
       "      <th>mean_count_proposition_per_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.376344</td>\n",
       "      <td>0.172043</td>\n",
       "      <td>0.043011</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.086022</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>7</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>4.407767</td>\n",
       "      <td>103</td>\n",
       "      <td>8.714286</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.330882</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.095588</td>\n",
       "      <td>0.066176</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.183824</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>9</td>\n",
       "      <td>18.111111</td>\n",
       "      <td>4.601227</td>\n",
       "      <td>163</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.111111</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.457364</td>\n",
       "      <td>0.131783</td>\n",
       "      <td>0.054264</td>\n",
       "      <td>0.077519</td>\n",
       "      <td>0.108527</td>\n",
       "      <td>0.170543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>17.375000</td>\n",
       "      <td>4.381295</td>\n",
       "      <td>139</td>\n",
       "      <td>10.125000</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.419048</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>0.104762</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>8</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.695312</td>\n",
       "      <td>128</td>\n",
       "      <td>8.875000</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.442623</td>\n",
       "      <td>0.057377</td>\n",
       "      <td>0.114754</td>\n",
       "      <td>0.065574</td>\n",
       "      <td>0.098361</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>0.040984</td>\n",
       "      <td>10</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>4.715447</td>\n",
       "      <td>123</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          E        M1        M2        M3        H1        H2        H3  \\\n",
       "0  0.376344  0.172043  0.043011  0.096774  0.086022  0.215054  0.010753   \n",
       "1  0.330882  0.176471  0.095588  0.066176  0.102941  0.183824  0.044118   \n",
       "2  0.457364  0.131783  0.054264  0.077519  0.108527  0.170543  0.000000   \n",
       "3  0.419048  0.142857  0.085714  0.019048  0.104762  0.209524  0.019048   \n",
       "4  0.442623  0.057377  0.114754  0.065574  0.098361  0.180328  0.040984   \n",
       "\n",
       "   count_sentence_per_text  mean_count_word_per_sentence  \\\n",
       "0                        7                     14.714286   \n",
       "1                        9                     18.111111   \n",
       "2                        8                     17.375000   \n",
       "3                        8                     16.000000   \n",
       "4                       10                     12.300000   \n",
       "\n",
       "   count_alphabet_per_word  count_word_per_text  \\\n",
       "0                 4.407767                  103   \n",
       "1                 4.601227                  163   \n",
       "2                 4.381295                  139   \n",
       "3                 4.695312                  128   \n",
       "4                 4.715447                  123   \n",
       "\n",
       "   mean_logest_word_per_sentence  mean_count_verb_per_sentence  \\\n",
       "0                       8.714286                      2.000000   \n",
       "1                      10.000000                      3.111111   \n",
       "2                      10.125000                      3.375000   \n",
       "3                       8.875000                      1.875000   \n",
       "4                       9.100000                      2.400000   \n",
       "\n",
       "   mean_count_proposition_per_sentence  \n",
       "0                             2.857143  \n",
       "1                             2.666667  \n",
       "2                             3.000000  \n",
       "3                             2.250000  \n",
       "4                             1.800000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# target = data_set['level']\n",
    "display(target)\n",
    "# data = data_set.drop(['level'],axis=1)\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c9745-6935-498e-8770-f1a87949fb72",
   "metadata": {},
   "source": [
    "## train, test set 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f5d3d4e-2036-46e7-a072-f9e223765705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test set 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.2, random_state=42, stratify=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0236e6a6-154f-4164-9368-0908c2c515e1",
   "metadata": {},
   "source": [
    "---\n",
    "## Optuna Hyper-Parameter Tuning 결과 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c06204e-7fdb-4d6b-8aa8-585993fdca14",
   "metadata": {},
   "source": [
    "### autoML로 나온 베스트 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b19f1d26-5de7-4a69-b0ef-d7f10bb53f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "rf = RandomForestClassifier(bootstrap=True,\n",
    "                            ccp_alpha=0.0,\n",
    "                            class_weight=None,\n",
    "                        criterion='gini',\n",
    "                            max_depth=5,\n",
    "                            max_features='auto',\n",
    "                        max_leaf_nodes=None,\n",
    "                            max_samples=None,\n",
    "                        min_impurity_decrease=0.0,\n",
    "                            min_impurity_split=None,\n",
    "                        min_samples_leaf=1,\n",
    "                            min_samples_split=2,\n",
    "                        min_weight_fraction_leaf=0.0,\n",
    "                            n_estimators=100,\n",
    "                        n_jobs=-1, oob_score=False,\n",
    "                            random_state=42,\n",
    "                            verbose=0,\n",
    "                        warm_start=False)\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e39e1d-c1fb-4553-98fc-5f84081f082f",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터 처리 후 0.63.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a17fc564-dfc8-4b1b-95ae-96b31cfa22eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_leaf_nodes': 9, 'max_features': 0.6011150117432088, 'max_depth': 4\n",
    "\n",
    "# # from sklearn.model_selection import StratifiedKFold\n",
    "# rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "#                         criterion='gini', max_depth=4, max_features='auto',\n",
    "#                         max_leaf_nodes=9, max_samples=None,\n",
    "#                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#                         min_samples_leaf=2, min_samples_split=3,\n",
    "#                         min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "#                         n_jobs=-1, oob_score=False, random_state=42, verbose=0,\n",
    "#                         warm_start=False)\n",
    "# rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2bed17-6291-4d68-9b5b-90a529a2db86",
   "metadata": {},
   "source": [
    "## Accuracy, Predict_proba, f1_score, CM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f825ea-d3b3-44b1-a7f8-f42cb242f92b",
   "metadata": {},
   "source": [
    "### validation 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c04da0b-4fb1-4d3d-9224-4925a4c05bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.95      0.93        22\n",
      "           2       0.79      0.90      0.84        21\n",
      "           3       0.89      0.76      0.82        21\n",
      "           4       0.73      0.57      0.64       104\n",
      "           5       0.00      0.00      0.00        58\n",
      "           6       0.61      0.94      0.74       151\n",
      "\n",
      "    accuracy                           0.68       377\n",
      "   macro avg       0.66      0.69      0.66       377\n",
      "weighted avg       0.59      0.68      0.62       377\n",
      "\n",
      "accuracy :\n",
      " 0.6816976127320955\n",
      "accuracy :\n",
      " 0.6816976127320955\n",
      "predict_proba :\n",
      " [[1.18463926e-01 3.71539747e-01 2.41858325e-01 1.49328154e-01\n",
      "  5.05055205e-02 6.83043278e-02]\n",
      " [9.11572931e-04 3.80737626e-04 5.14321920e-04 8.71225812e-02\n",
      "  1.44113552e-01 7.66957235e-01]\n",
      " [9.16479289e-03 9.90554073e-03 1.28750257e-02 3.01838455e-01\n",
      "  2.50654455e-01 4.15561730e-01]\n",
      " ...\n",
      " [6.51319042e-03 7.29359591e-02 1.54552624e-01 4.43955985e-01\n",
      "  1.40946146e-01 1.81096096e-01]\n",
      " [1.73940463e-03 2.11952110e-02 3.76690121e-02 4.23386789e-01\n",
      "  1.26088809e-01 3.89920774e-01]\n",
      " [4.38726283e-03 7.16848389e-03 1.13905992e-02 3.69285278e-01\n",
      "  2.22017151e-01 3.85751225e-01]]\n",
      "f1_score\n",
      " : 0.6209399234410692\n",
      "cm : \n",
      " [[ 21   1   0   0   0   0]\n",
      " [  1  19   0   1   0   0]\n",
      " [  1   4  16   0   0   0]\n",
      " [  0   0   1  59   0  44]\n",
      " [  0   0   1  12   0  45]\n",
      " [  0   0   0   9   0 142]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# RandomForest accuracy\n",
    "print('accuracy :\\n',rf.score(X_test,y_test))\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 다른 문법에서 같은 점수인지 확인\n",
    "print('accuracy :\\n',accuracy_score(y_test, y_pred))\n",
    "\n",
    "# predict_proba\n",
    "print('predict_proba :\\n',rf.predict_proba(X_train))\n",
    "\n",
    "# F1 score\n",
    "# F1 = 2 * (precision * recall) / (precision + recall)\n",
    "# from sklearn.metrics import f1_score\n",
    "print('f1_score\\n :',f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# CM\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "print('cm : \\n',confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d97034-a9ff-4600-9342-f37aaf0893f7",
   "metadata": {},
   "source": [
    "> 고2를 찾지를 못함.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a42bde-2327-4dd3-afb0-c7ef087cd77e",
   "metadata": {},
   "source": [
    "---\n",
    "## CV적용 모델 평가 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d6dd594-9b6a-4740-b5f8-9d908d7816a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg test score : 0.6959015745809585 ( +/- 0.021611382432968115)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "rkfold = RepeatedKFold(n_splits=5, random_state=42, n_repeats=10)\n",
    "\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "cross_val = cross_validate(\n",
    "    estimator = model,\n",
    "    X = train, y=target,\n",
    "    cv=rkfold\n",
    ")\n",
    "\n",
    "print('avg test score : {} ( +/- {})'.format(cross_val['test_score'].mean(), cross_val['test_score'].std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4858005-6386-45d3-8e24-2b0d2e9b1ba3",
   "metadata": {},
   "source": [
    "### 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e46cc1a6-daa0-47c5-ba4e-91274d40d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_classes = 6\n",
    "# n_estimators = 30\n",
    "# cmap = plt.cm.RdYlBu\n",
    "# plot_step = 0.02\n",
    "# plot_step_coarser = 0.5\n",
    "# RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f6225bf-dcf6-447f-8ce2-69a06da69a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_idx = 1\n",
    "# models = [RandomForestClassifier(),\n",
    "#          ExtraTreesClassifier(),\n",
    "#          DecisionTreeClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c68c37d-7250-4fdc-b522-39b010f14426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5f7888d-1683-4ec5-9e07-1c1635ce3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,8))\n",
    "\n",
    "# for pair in ([0,1],[0,2],[2,3]):\n",
    "    \n",
    "#     for model in models :\n",
    "        \n",
    "#         y = target\n",
    "#         X = train[3:7]\n",
    "        \n",
    "#         idx = np.arange(X.shape[0])\n",
    "#         np.random.seed(42)\n",
    "#         np.random.shuffle(idx)\n",
    "#         X = X[idx]\n",
    "#         y = y[idx]\n",
    "        \n",
    "#         mean = X.mean(axis=0)\n",
    "#         std = X.std(axis=0)\n",
    "#         X = (X - mean) / std\n",
    "        \n",
    "#         model.fit(X,y)\n",
    "        \n",
    "#         model_title = str(type(model)).split(\".\")[-1][:-2][:-len(\"Classifier\")]\n",
    "        \n",
    "#         plt.subplot(3,3,plot_idx)\n",
    "#         if plot_idx <= len(models):\n",
    "#             plt.title(model_title, fontsize=9)\n",
    "            \n",
    "#         x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
    "#         y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
    "#         xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "#                             np.arange(y_min, y_max, plot_step))\n",
    "        \n",
    "#         if isinstance(model, DecisionTreeClassifier):\n",
    "#             Z = model(predict(np.c_[xx.ravel(), yy.ravel()]))\n",
    "#             Z = Z.reshape(xx.shape)\n",
    "#             cs = plt.contourf(xx, yy, Z, cmap = cmap)\n",
    "#         else : \n",
    "#             estimator_alpha = 1.0 / len(model.estimators_)\n",
    "#             for tree in model.estimators_:\n",
    "#                 Z = model(predict(np.c_[xx.ravel(), yy.ravel()]))\n",
    "#                 Z = Z.reshape(xx.shape)\n",
    "#                 cs = plt.contourf(xx, yy, Z, alpha = estimaotr, cmap = cmap)\n",
    "\n",
    "#         xx_coarser, yy_coarser = np.meshgird(np.arange(x_min, x_max, plot_step_coarser),\n",
    "#                                             np.arange(y_min, y_max, plot_step_coarser))\n",
    "    \n",
    "#         cs_points = plt.scatter(xx_coarser, yy_coarser, s=15,\n",
    "#                                cmap=cmap, edgecolor='none')\n",
    "    \n",
    "#         plt.scatter(X[:,0], X[:,1], c=y,\n",
    "#                     cmap=ListedColormap(['r','g']),\n",
    "#                     edgecolor='k', s=20)\n",
    "#         plot_idx +=1\n",
    "    \n",
    "# plt.suptitle('Classifiers', font_size=12)\n",
    "# plt.axis('tight')\n",
    "# plt.tight_layout(h_pad=0.2, w_pad=0.2, pad=2.5)\n",
    "# plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52623b62-f7fe-42aa-b09a-5bec4efd52aa",
   "metadata": {},
   "source": [
    "---\n",
    "# ExtraTreesClassifier\n",
    "> accuracy를 비롯한 분류문제 값 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7de684f-89df-4d9e-9b06-04ed4536fff8",
   "metadata": {},
   "source": [
    "> 수치형 변수만 존재 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08448efd-4996-4aa0-9f30-13c8dff6c25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1881 entries, 0 to 1880\n",
      "Data columns (total 14 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   E                                    1881 non-null   float64\n",
      " 1   M1                                   1881 non-null   float64\n",
      " 2   M2                                   1881 non-null   float64\n",
      " 3   M3                                   1881 non-null   float64\n",
      " 4   H1                                   1881 non-null   float64\n",
      " 5   H2                                   1881 non-null   float64\n",
      " 6   H3                                   1881 non-null   float64\n",
      " 7   count_sentence_per_text              1881 non-null   int64  \n",
      " 8   mean_count_word_per_sentence         1881 non-null   float64\n",
      " 9   count_alphabet_per_word              1881 non-null   float64\n",
      " 10  count_word_per_text                  1881 non-null   int64  \n",
      " 11  mean_logest_word_per_sentence        1881 non-null   float64\n",
      " 12  mean_count_verb_per_sentence         1881 non-null   float64\n",
      " 13  mean_count_proposition_per_sentence  1881 non-null   float64\n",
      "dtypes: float64(12), int64(2)\n",
      "memory usage: 205.9 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       6\n",
       "1       6\n",
       "2       6\n",
       "3       6\n",
       "4       6\n",
       "       ..\n",
       "1876    6\n",
       "1877    6\n",
       "1878    6\n",
       "1879    6\n",
       "1880    6\n",
       "Name: level, Length: 1881, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.info()\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df25f7ab-4014-452e-b61c-55edc0f4d311",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ab092-b5f9-469b-8a24-ed9f05751cfe",
   "metadata": {},
   "source": [
    "## train, valid set 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1905334-be5a-4427-8fb7-fb10df63b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test set 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.2, random_state=42, stratify=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45a42b4-0410-4306-a284-3da9ebc3a12f",
   "metadata": {},
   "source": [
    "## Optuna Hyper-Parameter Tuning 결과 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53489d30-b5bf-43cc-a128-6fcdf3ace4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(random_state=42)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et = ExtraTreesClassifier(random_state=42)\n",
    "et.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a31c0b5-0c68-4086-b1eb-df993d58d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = make_classification(n_features=4, random_state=42)\n",
    "# et = ExtraTreesClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "#                         criterion='gini', max_depth=None, max_features='auto',\n",
    "#                         max_leaf_nodes=None, max_samples=None,\n",
    "#                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#                         min_samples_leaf=1, min_samples_split=2,\n",
    "#                         min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "#                         n_jobs=-1, oob_score=False, random_state=42, verbose=0,\n",
    "#                         warm_start=False)\n",
    "# et.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726985e8-1444-4066-825c-43172a54bacd",
   "metadata": {},
   "source": [
    "## Accuracy, Predict_proba, f1_score, CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75b63614-afde-4256-bf98-3a03c4ca2c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.95      0.95        22\n",
      "           2       0.75      0.86      0.80        21\n",
      "           3       0.79      0.71      0.75        21\n",
      "           4       0.72      0.68      0.70       104\n",
      "           5       0.58      0.12      0.20        58\n",
      "           6       0.69      0.91      0.78       151\n",
      "\n",
      "    accuracy                           0.72       377\n",
      "   macro avg       0.75      0.71      0.70       377\n",
      "weighted avg       0.70      0.72      0.68       377\n",
      "\n",
      "accuracy :\n",
      " 0.7161803713527851\n",
      "accuracy :\n",
      " 0.7161803713527851\n",
      "predict_proba :\n",
      " [[0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "f1_score\n",
      " : 0.6798315537019461\n",
      "cm : \n",
      " [[ 21   1   0   0   0   0]\n",
      " [  0  18   2   1   0   0]\n",
      " [  0   5  15   1   0   0]\n",
      " [  0   0   1  71   3  29]\n",
      " [  0   0   1  16   7  34]\n",
      " [  1   0   0  10   2 138]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = et.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# RandomForest accuracy\n",
    "print('accuracy :\\n',et.score(X_test,y_test))\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# 다른 문법에서 같은 점수인지 확인\n",
    "print('accuracy :\\n',accuracy_score(y_test, y_pred))\n",
    "\n",
    "# predict_proba\n",
    "print('predict_proba :\\n',et.predict_proba(X_train))\n",
    "\n",
    "# F1 score\n",
    "# F1 = 2 * (precision * recall) / (precision + recall)\n",
    "# from sklearn.metrics import f1_score\n",
    "print('f1_score\\n :',f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# CM\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "print('cm : \\n',confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992ac74-4eeb-4e53-81de-aed0521f35f0",
   "metadata": {},
   "source": [
    "## CV적용 모델 결과 시각화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fee27c7-b4dc-4bc5-a473-cbdb02416e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg test score : 0.696432642925673 ( +/- 0.021453923918561)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "rkfold = RepeatedKFold(n_splits=5, random_state=42, n_repeats=10)\n",
    "\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "cross_val = cross_validate(\n",
    "    estimator = model,\n",
    "    X = train, y=target,\n",
    "    cv=rkfold\n",
    ")\n",
    "\n",
    "print('avg test score : {} ( +/- {})'.format(cross_val['test_score'].mean(), cross_val['test_score'].std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d3056-9262-40db-99d5-6e58210c7601",
   "metadata": {},
   "source": [
    "---\n",
    "# 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1246d81-383a-4867-94b5-2e6eb69f070d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['E', 'M1', 'M2', 'M3', 'H1', 'H2', 'H3', 'count_sentence_per_text',\n",
       "       'mean_count_word_per_sentence', 'count_alphabet_per_word',\n",
       "       'count_word_per_text', 'mean_logest_word_per_sentence',\n",
       "       'mean_count_verb_per_sentence', 'mean_count_proposition_per_sentence',\n",
       "       'level'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e746868b-b2dd-42d3-bc38-599076932a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1881 entries, 0 to 1880\n",
      "Data columns (total 15 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   order                                1881 non-null   int64  \n",
      " 1   E                                    1881 non-null   float64\n",
      " 2   M1                                   1881 non-null   float64\n",
      " 3   M2                                   1881 non-null   float64\n",
      " 4   M3                                   1881 non-null   float64\n",
      " 5   H1                                   1881 non-null   float64\n",
      " 6   H2                                   1881 non-null   float64\n",
      " 7   H3                                   1881 non-null   float64\n",
      " 8   count_sentence_per_text              1881 non-null   int64  \n",
      " 9   mean_count_word_per_sentence         1881 non-null   float64\n",
      " 10  count_alphabet_per_word              1881 non-null   float64\n",
      " 11  mean_logest_word_per_sentence        1881 non-null   float64\n",
      " 12  mean_count_verb_per_sentence         1881 non-null   float64\n",
      " 13  mean_count_proposition_per_sentence  1881 non-null   float64\n",
      " 14  level                                1881 non-null   int64  \n",
      "dtypes: float64(12), int64(3)\n",
      "memory usage: 220.6 KB\n",
      "None \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order</th>\n",
       "      <th>E</th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>H1</th>\n",
       "      <th>H2</th>\n",
       "      <th>H3</th>\n",
       "      <th>count_sentence_per_text</th>\n",
       "      <th>mean_count_word_per_sentence</th>\n",
       "      <th>count_alphabet_per_word</th>\n",
       "      <th>mean_logest_word_per_sentence</th>\n",
       "      <th>mean_count_verb_per_sentence</th>\n",
       "      <th>mean_count_proposition_per_sentence</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1881.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>434.017012</td>\n",
       "      <td>0.372603</td>\n",
       "      <td>0.163700</td>\n",
       "      <td>0.075728</td>\n",
       "      <td>0.068157</td>\n",
       "      <td>0.090168</td>\n",
       "      <td>0.216683</td>\n",
       "      <td>0.012961</td>\n",
       "      <td>20.333865</td>\n",
       "      <td>15.424791</td>\n",
       "      <td>4.642195</td>\n",
       "      <td>9.277037</td>\n",
       "      <td>2.607641</td>\n",
       "      <td>2.572225</td>\n",
       "      <td>4.614567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>899.668766</td>\n",
       "      <td>0.079522</td>\n",
       "      <td>0.039693</td>\n",
       "      <td>0.031354</td>\n",
       "      <td>0.032102</td>\n",
       "      <td>0.040753</td>\n",
       "      <td>0.033358</td>\n",
       "      <td>0.016827</td>\n",
       "      <td>20.839410</td>\n",
       "      <td>5.969731</td>\n",
       "      <td>0.469391</td>\n",
       "      <td>1.670848</td>\n",
       "      <td>1.068035</td>\n",
       "      <td>1.320709</td>\n",
       "      <td>1.474889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.180723</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.119658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.580645</td>\n",
       "      <td>3.505076</td>\n",
       "      <td>5.338710</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.311033</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.061713</td>\n",
       "      <td>0.194690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>4.290780</td>\n",
       "      <td>8.074074</td>\n",
       "      <td>1.883333</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.368000</td>\n",
       "      <td>0.162338</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.065574</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.214555</td>\n",
       "      <td>0.007874</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>15.435484</td>\n",
       "      <td>4.612245</td>\n",
       "      <td>9.285714</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>435.000000</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.189759</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.116788</td>\n",
       "      <td>0.236994</td>\n",
       "      <td>0.017391</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>19.454545</td>\n",
       "      <td>4.973510</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>3.272727</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9591.000000</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.329897</td>\n",
       "      <td>0.221429</td>\n",
       "      <td>0.226804</td>\n",
       "      <td>0.232759</td>\n",
       "      <td>0.373494</td>\n",
       "      <td>0.116071</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>47.666667</td>\n",
       "      <td>6.230769</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             order            E           M1           M2           M3  \\\n",
       "count  1881.000000  1881.000000  1881.000000  1881.000000  1881.000000   \n",
       "mean    434.017012     0.372603     0.163700     0.075728     0.068157   \n",
       "std     899.668766     0.079522     0.039693     0.031354     0.032102   \n",
       "min       6.000000     0.180723     0.035714     0.000000     0.000000   \n",
       "25%      28.000000     0.311033     0.136364     0.055556     0.045455   \n",
       "50%      55.000000     0.368000     0.162338     0.073529     0.065574   \n",
       "75%     435.000000     0.430108     0.189759     0.094017     0.087719   \n",
       "max    9591.000000     0.638298     0.329897     0.221429     0.226804   \n",
       "\n",
       "                H1           H2           H3  count_sentence_per_text  \\\n",
       "count  1881.000000  1881.000000  1881.000000              1881.000000   \n",
       "mean      0.090168     0.216683     0.012961                20.333865   \n",
       "std       0.040753     0.033358     0.016827                20.839410   \n",
       "min       0.004808     0.119658     0.000000                 3.000000   \n",
       "25%       0.061713     0.194690     0.000000                 7.000000   \n",
       "50%       0.089286     0.214555     0.007874                10.000000   \n",
       "75%       0.116788     0.236994     0.017391                29.000000   \n",
       "max       0.232759     0.373494     0.116071               138.000000   \n",
       "\n",
       "       mean_count_word_per_sentence  count_alphabet_per_word  \\\n",
       "count                   1881.000000              1881.000000   \n",
       "mean                      15.424791                 4.642195   \n",
       "std                        5.969731                 0.469391   \n",
       "min                        3.580645                 3.505076   \n",
       "25%                       10.900000                 4.290780   \n",
       "50%                       15.435484                 4.612245   \n",
       "75%                       19.454545                 4.973510   \n",
       "max                       47.666667                 6.230769   \n",
       "\n",
       "       mean_logest_word_per_sentence  mean_count_verb_per_sentence  \\\n",
       "count                    1881.000000                   1881.000000   \n",
       "mean                        9.277037                      2.607641   \n",
       "std                         1.670848                      1.068035   \n",
       "min                         5.338710                      0.307692   \n",
       "25%                         8.074074                      1.883333   \n",
       "50%                         9.285714                      2.500000   \n",
       "75%                        10.500000                      3.272727   \n",
       "max                        15.600000                      8.200000   \n",
       "\n",
       "       mean_count_proposition_per_sentence        level  \n",
       "count                          1881.000000  1881.000000  \n",
       "mean                              2.572225     4.614567  \n",
       "std                               1.320709     1.474889  \n",
       "min                               0.088235     1.000000  \n",
       "25%                               1.571429     4.000000  \n",
       "50%                               2.500000     5.000000  \n",
       "75%                               3.428571     6.000000  \n",
       "max                              11.666667     6.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "with open('./data/1004_sample_with_feature.csv', encoding=\"UTF-8\") as f:\n",
    "    table = pd.read_csv(f) # csv 읽어오기\n",
    "data_set = pd.DataFrame(table) # df 로 변환\n",
    "data_set = data_set.drop('count_word_per_text',axis=1) # 의미없는 col 삭제\n",
    "data_set = data_set.fillna(0) # 결측치 0처리 = 단어장에서 고등학교 단어들\n",
    "data_set = data_set.replace({'H3':6,'H2': 5,'H1': 4,'M3': 3,'M2': 2,'M1':1 })\n",
    "print(data_set.info(),'\\n')\n",
    "display(data_set.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5e66b7c-f63d-4879-9447-910185420d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b8d610a-4453-4751-a38b-84e86a3e8da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       6\n",
       "1       6\n",
       "2       6\n",
       "3       6\n",
       "4       6\n",
       "       ..\n",
       "1876    6\n",
       "1877    6\n",
       "1878    6\n",
       "1879    6\n",
       "1880    6\n",
       "Name: level, Length: 1881, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order</th>\n",
       "      <th>E</th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>H1</th>\n",
       "      <th>H2</th>\n",
       "      <th>H3</th>\n",
       "      <th>count_sentence_per_text</th>\n",
       "      <th>mean_count_word_per_sentence</th>\n",
       "      <th>count_alphabet_per_word</th>\n",
       "      <th>mean_logest_word_per_sentence</th>\n",
       "      <th>mean_count_verb_per_sentence</th>\n",
       "      <th>mean_count_proposition_per_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>0.376344</td>\n",
       "      <td>0.172043</td>\n",
       "      <td>0.043011</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.086022</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>7</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>4.407767</td>\n",
       "      <td>8.714286</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>0.330882</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.095588</td>\n",
       "      <td>0.066176</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.183824</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>9</td>\n",
       "      <td>18.111111</td>\n",
       "      <td>4.601227</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.111111</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>0.457364</td>\n",
       "      <td>0.131783</td>\n",
       "      <td>0.054264</td>\n",
       "      <td>0.077519</td>\n",
       "      <td>0.108527</td>\n",
       "      <td>0.170543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>17.375000</td>\n",
       "      <td>4.381295</td>\n",
       "      <td>10.125000</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>0.419048</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>0.104762</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>8</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.695312</td>\n",
       "      <td>8.875000</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>0.442623</td>\n",
       "      <td>0.057377</td>\n",
       "      <td>0.114754</td>\n",
       "      <td>0.065574</td>\n",
       "      <td>0.098361</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>0.040984</td>\n",
       "      <td>10</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>4.715447</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order         E        M1        M2        M3        H1        H2  \\\n",
       "0     28  0.376344  0.172043  0.043011  0.096774  0.086022  0.215054   \n",
       "1     45  0.330882  0.176471  0.095588  0.066176  0.102941  0.183824   \n",
       "2     36  0.457364  0.131783  0.054264  0.077519  0.108527  0.170543   \n",
       "3     36  0.419048  0.142857  0.085714  0.019048  0.104762  0.209524   \n",
       "4     55  0.442623  0.057377  0.114754  0.065574  0.098361  0.180328   \n",
       "\n",
       "         H3  count_sentence_per_text  mean_count_word_per_sentence  \\\n",
       "0  0.010753                        7                     14.714286   \n",
       "1  0.044118                        9                     18.111111   \n",
       "2  0.000000                        8                     17.375000   \n",
       "3  0.019048                        8                     16.000000   \n",
       "4  0.040984                       10                     12.300000   \n",
       "\n",
       "   count_alphabet_per_word  mean_logest_word_per_sentence  \\\n",
       "0                 4.407767                       8.714286   \n",
       "1                 4.601227                      10.000000   \n",
       "2                 4.381295                      10.125000   \n",
       "3                 4.695312                       8.875000   \n",
       "4                 4.715447                       9.100000   \n",
       "\n",
       "   mean_count_verb_per_sentence  mean_count_proposition_per_sentence  \n",
       "0                      2.000000                             2.857143  \n",
       "1                      3.111111                             2.666667  \n",
       "2                      3.375000                             3.000000  \n",
       "3                      1.875000                             2.250000  \n",
       "4                      2.400000                             1.800000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target = train['level']\n",
    "display(target)\n",
    "train = train.drop(['level'],axis=1)\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d62bfbd-ebd0-48c6-8d1f-8d205b8ed8ad",
   "metadata": {},
   "source": [
    "## log_loss값 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d40cd0a-76bf-4fe4-b0bb-d001a332b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c, t in zip(train.dtypes.index, train.dtypes)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4043e68-3674-4d2b-964c-8481d65a67f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_cols = []\n",
    "for col in cols:\n",
    "    ohe_cols.append(pd.get_dummies(train[col], prefix=col, prefix_sep='-'))\n",
    "\n",
    "train = train.join(ohe_cols)\n",
    "train.drop(cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbc6feef-3a8e-4d5a-a728-05a7c07f8e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test set 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.2, random_state=42, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ec03817-2d8e-4521-94be-67632936a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False) # log_loss값을 구하기위해\n",
    "target_ohe = ohe.fit_transform(target.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a382bbd7-3b55-4938-983a-7934983742b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def voting_model_test(model_lst, X,y):\n",
    "    valid_scores = [[] for i in range(len(model_lst))]\n",
    "    voting_scores = []\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for tr_idx, va_idx in skf.split(X,y):\n",
    "        X_train, X_val = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_train, y_val = y[tr_idx], y[va_idx]\n",
    "        preds = []\n",
    "        \n",
    "        for idx, model in enumerate(model_lst):\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict_proba(X_val)\n",
    "            valid_score = log_loss(ohe.transform(y_val.reshape(-1,1)),pred)\n",
    "            valid_scores[idx].append(valid_score)\n",
    "            preds.append(pred)\n",
    "        print(np.array(preds).shape)\n",
    "        pred = np.mean(preds, axis=0)\n",
    "        print(pred.shape)\n",
    "        \n",
    "        voting_score = log_loss(ohe.transform(y_val.reshape(-1,1)), pred)\n",
    "        print(voting_score)\n",
    "        voting_scores.append(voting_score)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return valid_scores, voting_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8791f192-0cc1-4bb8-aae5-4cd51ee34fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, precision_score, accuracy_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b91102d-3ee3-48d5-8e04-349e78a08b83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 377, 6)\n",
      "(377, 6)\n",
      "1.7515352471166026\n",
      "(2, 376, 6)\n",
      "(376, 6)\n",
      "1.5046974833742572\n",
      "(2, 376, 6)\n",
      "(376, 6)\n",
      "1.6899263865325065\n",
      "(2, 376, 6)\n",
      "(376, 6)\n",
      "1.6820285753961162\n",
      "(2, 376, 6)\n",
      "(376, 6)\n",
      "1.6334096738665713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[1.7719206006975796,\n",
       "   1.623155058924838,\n",
       "   1.8652626802517112,\n",
       "   1.7693493095953883,\n",
       "   1.667659317553498],\n",
       "  [1.9109050053542183,\n",
       "   1.7302463791507245,\n",
       "   1.8630362215465532,\n",
       "   1.9362297466703957,\n",
       "   1.9348518721880332]],\n",
       " [1.7515352471166026,\n",
       "  1.5046974833742572,\n",
       "  1.6899263865325065,\n",
       "  1.6820285753961162,\n",
       "  1.6334096738665713])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "et = ExtraTreesClassifier()\n",
    "\n",
    "model_lst = [rf, et]\n",
    "voting_model_test([rf,et], train, target.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1240d-b5cf-451a-9ed0-0a481505e5fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b813b7-0f2e-4220-8898-5e6c5cb7fa10",
   "metadata": {},
   "source": [
    "## voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "48010056-8f5b-4a04-a468-90268678a604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.40 (+/- 0.02) [RandomForestClassifier]\n",
      "Accuracy: 0.48 (+/- 0.03) [ExtraTreesClassifier]\n",
      "Accuracy: 0.48 (+/- 0.03) [VotingClassifier]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "rkfold = RepeatedKFold(n_splits=5, random_state=42, n_repeats=10)\n",
    "\n",
    "model1 = RandomForestClassifier(bootstrap=True,\n",
    "                            ccp_alpha=0.0,\n",
    "                            class_weight=None,\n",
    "                        criterion='gini',\n",
    "                            max_depth=5,\n",
    "                            max_features='auto',\n",
    "                        max_leaf_nodes=None,\n",
    "                            max_samples=None,\n",
    "                        min_impurity_decrease=0.0,\n",
    "                            min_impurity_split=None,\n",
    "                        min_samples_leaf=1,\n",
    "                            min_samples_split=2,\n",
    "                        min_weight_fraction_leaf=0.0,\n",
    "                            n_estimators=100,\n",
    "                        n_jobs=-1, oob_score=False,\n",
    "                            random_state=42,\n",
    "                            verbose=0,\n",
    "                        warm_start=False)\n",
    "model2 = ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight={},\n",
    "                     criterion='entropy', max_depth=9, max_features=1.0,\n",
    "                     max_leaf_nodes=None, max_samples=None,\n",
    "                     min_impurity_decrease=0.0005, min_impurity_split=None,\n",
    "                     min_samples_leaf=4, min_samples_split=10,\n",
    "                     min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
    "                     oob_score=False, random_state=42, verbose=0,\n",
    "                     warm_start=False)\n",
    "# model3 = DecisionTreeClassifier()\n",
    "vote_model = VotingClassifier(\n",
    "    estimators = [('rf', model1), ('et',model2)] , # ,('dt',model3)\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "for model in (model1, model2, vote_model): # , model3\n",
    "    model_name = str(type(model)).split('.')[-1][:-2]\n",
    "    scores = cross_val_score(model, train, target, cv=rkfold)\n",
    "    print('Accuracy: %0.2f (+/- %0.2f) [%s]' % (scores.mean(), scores.std(), model_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5754d49a-47cd-4050-83c0-71fb806c67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "# rkfold = RepeatedKFold(n_splits=5, random_state=42, n_repeats=10)\n",
    "# 스케일링 안들어감.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f74b8-8e51-477b-aaa7-4e712373c614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
