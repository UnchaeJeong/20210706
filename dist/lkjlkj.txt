문의사항이 있으시면 아래 연락처로 연락주세요! 
010-5036-2776 망넛이네 담당자님

{'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.272 Whale/2.9.118.16 Safari/537.36'}

서비스키 : 8pcbeee_4tj81o811t4r5ttot5j8ppre
이동데이터 요청 URL : https://open.jejudatahub.net/api/proxy/51tt9Daaa6atttDataaa5Dbttattttat/{your_projectKey}?{params(key=value)}
searchDate movingPath

https://open.jejudatahub.net/api/proxy/51tt9Daaa6atttDataaa5Dbttattttat/{}?{params(key=value)}
https://open.jejudatahub.net/api/proxy/51tt9Daaa6atttDataaa5Dbttattttat/{}?{params(key=value)}


import requests
from lxml import html
import time

keyword = 'pann'

output_file_name = keyword + "_list_" + time.strftime("%y%m%d_%H%M%S") + '.txt'
output_file = open(output_file_name, "w", encoding="utf-8")
output_file.write("{}\t{}\t{}\n".format('페이지', '제목', 'URL'))
output_file.close()


def fwrite_news(i, article_title, article_url):
    print([i, article_title, article_url])
    output_file = open(output_file_name, "a", encoding="utf-8")
    output_file.write("{}\t{}\t{}\n".format(i, article_title, article_url))
    return


def fcrawl_news(i):
    page_num = i

    url = 'https://pann.nate.com/talk/c20048?page='+str(page_num)
    print(url)

    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\
      AppleWebKit/537.36 (HTML, like Gecko) Chrome/86.0.4240.272 Whale/2.9.118.16 Safari/537.36'}
    html_req = requests.get(url, headers=headers)

    tree = html.fromstring(html_req.content)
    bodies = tree.xpath('//td[@class="subject"]')
    print(len(bodies))

    results = []

    for body in bodies:
        try:
            article_title = body.xpath('a/text()')[0]
        except:
            article_title = ''
        print(article_title)
        try:
            article_url = body.xpath('a/@href')[0]
            article_url = 'https://pann.nate.com'+article_url
        except:
            article_url = ''
        print(article_url)

        if article_title != '':
            article_title_clean = article_title.replace("\n", "").replace("\t", "").replace("\r", "").strip()
            results.append([article_title_clean, article_url])
            fwrite_news(i, article_title_clean, article_url)

    output_file.close()

    return results


def fmain():

    for i in range(1, 3):
        print(i)
        results = fcrawl_news(i)
        print(results)
        time.sleep(6)


fmain()